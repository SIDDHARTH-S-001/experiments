{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium import ObservationWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributions as distributions\n",
    "from torch.distributions import Categorical\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, action_size) -> None:\n",
    "        super(Network, self).__init__() # Activate inheritance\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=4,out_channels=32, kernel_size=(3, 3), stride=2) # we will have a stack of 4 grey scale images\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32,out_channels=32, kernel_size=(3, 3), stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=32,out_channels=32, kernel_size=(3, 3), stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        # 1st fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(512, 128)\n",
    "        # we'll have 2 output layers, 1 for the action values (actor) and other for state value (critic)\n",
    "        self.fc2a = torch.nn.Linear(128, action_size)\n",
    "        self.fc2s = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        action_values = self.fc2a(x)\n",
    "        state_value = self.fc2s(x)[0] # gets the value instead of the whole array\n",
    "\n",
    "        return action_values, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4, 42, 42)\n",
      "Number actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment KungFuMasterDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/usr/local/lib/python3.8/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.get_action_meanings to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.get_action_meanings` for environment variables or `env.get_wrapper_attr('get_action_meanings')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "class PreprocessAtari(ObservationWrapper):\n",
    "\n",
    "  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
    "    super(PreprocessAtari, self).__init__(env)\n",
    "    self.img_size = (height, width)\n",
    "    self.crop = crop\n",
    "    self.dim_order = dim_order\n",
    "    self.color = color\n",
    "    self.frame_stack = n_frames\n",
    "    n_channels = 3 * n_frames if color else n_frames\n",
    "    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
    "    self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "    self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    self.frames = np.zeros_like(self.frames)\n",
    "    obs, info = self.env.reset()\n",
    "    self.update_buffer(obs)\n",
    "    return self.frames, info\n",
    "\n",
    "  def observation(self, img):\n",
    "    img = self.crop(img)\n",
    "    img = cv2.resize(img, self.img_size)\n",
    "    if not self.color:\n",
    "      if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype('float32') / 255.\n",
    "    if self.color:\n",
    "      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
    "    else:\n",
    "      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
    "    if self.color:\n",
    "      self.frames[-3:] = img\n",
    "    else:\n",
    "      self.frames[-1] = img\n",
    "    return self.frames\n",
    "\n",
    "  def update_buffer(self, obs):\n",
    "    self.frames = self.observation(obs)\n",
    "\n",
    "def make_env():\n",
    "  env = gym.make(\"KungFuMasterDeterministic-v0\", render_mode = 'rgb_array')\n",
    "  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
    "  return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "number_actions = env.action_space.n\n",
    "print(\"State shape:\", state_shape)\n",
    "print(\"Number actions:\", number_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "discount_factor = 0.9\n",
    "number_environments = 10 # basically 10 different agents working seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing A3C class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        # There is only one network, unlike DQN and DCQN which had a local and target network\n",
    "        self.network = Network(action_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        # we always need an extra dimension for the batches\n",
    "        if state.ndim == 3:\n",
    "            state = [state] # because state variable is not a pytorch tensor yet\n",
    "        # now convert the state variable to a torch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        action_values, _ = self.network(state)\n",
    "        # implement Softman policy as the action-selection strategy\n",
    "        policy = F.softmax(action_values, dim=-1)\n",
    "\n",
    "        return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu().numpy()])\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done): # will be done in batches\n",
    "        batch_size = state.shape[0]\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=self.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)\n",
    "        done = torch.tensor(done, dtype=torch.bool, device=self.device).to(torch.float32)\n",
    "        action_values, state_value = self.network(state)\n",
    "        _, next_state_value = self.network(next_state) # returns next action and next state, but we are not interested in the next action\n",
    "        \n",
    "        # Bellman equation\n",
    "        target_state_value = reward + discount_factor * next_state_value * (1 - done)\n",
    "        \n",
    "        # Implement Advantage\n",
    "        advamtage = target_state_value - state_value \n",
    "        \n",
    "        # actor loss calculation \n",
    "        probs = F.softmax(action_values, dim=-1)\n",
    "        logprobs = F.log_softmax(action_values, dim=-1)\n",
    "        entropy = -torch.sum(probs * logprobs, axis=-1)\n",
    "        batch_idx = np.arange(batch_size)\n",
    "        logp_actions = logprobs[batch_idx, action]\n",
    "        actor_loss = -(logp_actions * advamtage.detach()).mean() - 0.001 * entropy.mean() # the small value is multiplied to balance the importance of the entropy\n",
    "        \n",
    "        # critic loss (mse loss between target state value and the state value)\n",
    "        critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the A3C Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size=number_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaulate agent on one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_episodes = 1):\n",
    "    episodes_rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        while True: # seuence of instructions for agent\n",
    "            # step 1 -> play an action\n",
    "            action = agent.act(state)\n",
    "            # step 2 -> get the next state, reward and status (done or not) and update total reward\n",
    "            state, reward, done, info, _ = env.step(action[0])\n",
    "            total_reward += reward\n",
    "            # step 3 -> if done then finish episode\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episodes_rewards.append(total_reward)\n",
    "    \n",
    "    return episodes_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing Multiple Environments Simultaenously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvBatch:\n",
    "    def __init__(self, n_envs = 10): # initializing multiple environments\n",
    "        self.envs = [make_env() for _ in range(n_envs)]\n",
    "\n",
    "    def reset(self): # resetting multiple environments\n",
    "        _states = []\n",
    "        for env in self.envs:\n",
    "            _states.append(env.reset()[0])\n",
    "        \n",
    "        return np.array(_states)\n",
    "    \n",
    "    def step(self, actions): # stepping multiple agents in multiple environments\n",
    "        next_states, rewards, dones, infos, _ = map(np.array, zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
    "        # check if any environment is finished\n",
    "        for i in range(len(self.envs)):\n",
    "            if dones[i]:\n",
    "                next_states = self.envs[i].reset()[0] # we only want the state\n",
    "        \n",
    "        return next_states, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training multiple agents in multiple environments (Asynchronous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]/tmp/ipykernel_4345/3270161148.py:45: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
      "  1%|          | 19/3001 [00:11<21:10,  2.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward:  480.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 913/3001 [00:17<00:39, 52.55it/s] \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x16 and 512x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m batch_next_states, batch_rewards, batch_dones, _ \u001b[38;5;241m=\u001b[39m env_batch\u001b[38;5;241m.\u001b[39mstep(batch_actions)\n\u001b[1;32m     10\u001b[0m batch_rewards \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_actions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_rewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_next_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_dones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m batch_states \u001b[38;5;241m=\u001b[39m batch_next_states\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[56], line 28\u001b[0m, in \u001b[0;36mAgent.step\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     26\u001b[0m done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(done, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbool, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     27\u001b[0m action_values, state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(state)\n\u001b[0;32m---> 28\u001b[0m _, next_state_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# returns next action and next state, but we are not interested in the next action\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Bellman equation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m target_state_value \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m discount_factor \u001b[38;5;241m*\u001b[39m next_state_value \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m done)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 22\u001b[0m, in \u001b[0;36mNetwork.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     20\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(x)\n\u001b[0;32m---> 22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     24\u001b[0m action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2a(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x16 and 512x128)"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "env_batch = EnvBatch(number_environments)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "with tqdm.trange(0, 3001) as progress_bar:\n",
    "  for i in progress_bar:\n",
    "    batch_actions = agent.act(batch_states)\n",
    "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
    "    batch_rewards *= 0.01\n",
    "    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
    "    batch_states = batch_next_states\n",
    "    if i % 1000 == 0:\n",
    "      print(\"Average agent reward: \", np.mean(evaluate(agent, env, n_episodes = 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
