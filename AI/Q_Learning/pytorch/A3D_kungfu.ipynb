{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium import ObservationWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributions as distributions\n",
    "from torch.distributions import Categorical\n",
    "import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, action_size) -> None:\n",
    "        super(Network, self).__init__() # Activate inheritance\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=4,out_channels=32, kernel_size=(3, 3), stride=2) # we will have a stack of 4 grey scale images\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32,out_channels=32, kernel_size=(3, 3), stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=32,out_channels=32, kernel_size=(3, 3), stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        # 1st fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(32*4*4, 128)\n",
    "        # we'll have 2 output layers, 1 for the action values (actor) and other for state value (critic)\n",
    "        self.fc2a = torch.nn.Linear(128, action_size)\n",
    "        self.fc2s = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        action_values = self.fc2a(x)\n",
    "        state_value = self.fc2s(x)[0] # gets the value instead of the whole array\n",
    "        return action_values, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape: (4, 42, 42)\n",
      "Number actions: 14\n",
      "Action names: ['NOOP', 'UP', 'RIGHT', 'LEFT', 'DOWN', 'DOWNRIGHT', 'DOWNLEFT', 'RIGHTFIRE', 'LEFTFIRE', 'DOWNFIRE', 'UPRIGHTFIRE', 'UPLEFTFIRE', 'DOWNRIGHTFIRE', 'DOWNLEFTFIRE']\n"
     ]
    }
   ],
   "source": [
    "class PreprocessAtari(ObservationWrapper):\n",
    "\n",
    "  def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
    "    super(PreprocessAtari, self).__init__(env)\n",
    "    self.img_size = (height, width)\n",
    "    self.crop = crop\n",
    "    self.dim_order = dim_order\n",
    "    self.color = color\n",
    "    self.frame_stack = n_frames\n",
    "    n_channels = 3 * n_frames if color else n_frames\n",
    "    obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (n_channels, height, width)}[dim_order]\n",
    "    self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "    self.frames = np.zeros(obs_shape, dtype = np.float32)\n",
    "\n",
    "  def reset(self):\n",
    "    self.frames = np.zeros_like(self.frames)\n",
    "    obs, info = self.env.reset()\n",
    "    self.update_buffer(obs)\n",
    "    return self.frames, info\n",
    "\n",
    "  def observation(self, img):\n",
    "    img = self.crop(img)\n",
    "    img = cv2.resize(img, self.img_size)\n",
    "    if not self.color:\n",
    "      if len(img.shape) == 3 and img.shape[2] == 3:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = img.astype('float32') / 255.\n",
    "    if self.color:\n",
    "      self.frames = np.roll(self.frames, shift = -3, axis = 0)\n",
    "    else:\n",
    "      self.frames = np.roll(self.frames, shift = -1, axis = 0)\n",
    "    if self.color:\n",
    "      self.frames[-3:] = img\n",
    "    else:\n",
    "      self.frames[-1] = img\n",
    "    return self.frames\n",
    "\n",
    "  def update_buffer(self, obs):\n",
    "    self.frames = self.observation(obs)\n",
    "\n",
    "def make_env():\n",
    "  env = gym.make(\"KungFuMasterDeterministic-v0\", render_mode = 'rgb_array')\n",
    "  env = PreprocessAtari(env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4)\n",
    "  return env\n",
    "\n",
    "env = make_env()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "number_actions = env.action_space.n\n",
    "print(\"State shape:\", state_shape)\n",
    "print(\"Number actions:\", number_actions)\n",
    "print(\"Action names:\", env.env.env.get_action_meanings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "discount_factor = 0.9\n",
    "number_environments = 10 # basically 10 different agents working seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing A3C class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        # There is only one network, unlike DQN and DCQN which had a local and target network\n",
    "        self.network = Network(action_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        # we always need an extra dimension for the batches\n",
    "        if state.ndim == 3:\n",
    "            state = [state] # because state variable is not a pytorch tensor yet\n",
    "        # now convert the state variable to a torch tensor\n",
    "        state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
    "        action_values, _ = self.network(state)\n",
    "        # implement Softman policy as the action-selection strategy\n",
    "        policy = F.softmax(action_values, dim = -1)\n",
    "\n",
    "        return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu().numpy()])\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done): # will be done in batches\n",
    "        batch_size = state.shape[0]\n",
    "        state = torch.tensor(state, dtype = torch.float32, device = self.device)\n",
    "        next_state = torch.tensor(next_state, dtype = torch.float32, device = self.device)\n",
    "        reward = torch.tensor(reward, dtype = torch.float32, device = self.device)\n",
    "        done = torch.tensor(done, dtype = torch.bool, device = self.device).to(dtype = torch.float32)\n",
    "        action_values, state_value = self.network(state)\n",
    "        _, next_state_value = self.network(next_state) # returns next action and next state, but we are not interested in the next action\n",
    "        \n",
    "        # Bellman equation\n",
    "        target_state_value = reward + discount_factor * next_state_value * (1 - done)\n",
    "        \n",
    "        # Implement Advantage\n",
    "        advantage = target_state_value - state_value \n",
    "        \n",
    "        # actor loss calculation \n",
    "        probs = F.softmax(action_values, dim = -1)\n",
    "        logprobs = F.log_softmax(action_values, dim = -1)\n",
    "        entropy = -torch.sum(probs * logprobs, axis = -1)\n",
    "        batch_idx = np.arange(batch_size)\n",
    "        logp_actions = logprobs[batch_idx, action]\n",
    "        actor_loss = -(logp_actions * advantage.detach()).mean() - 0.001 * entropy.mean() # the small value is multiplied to balance the importance of the entropy\n",
    "        \n",
    "        # critic loss (mse loss between target state value and the state value)\n",
    "        critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the A3C Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(number_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaulate agent on one episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(agent, env, n_episodes = 1):\n",
    "    episodes_rewards = []\n",
    "    for _ in range(n_episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        while True: # seuence of instructions for agent\n",
    "            # step 1 -> play an action\n",
    "            action = agent.act(state)\n",
    "            # step 2 -> get the next state, reward and status (done or not) and update total reward\n",
    "            state, reward, done, info, _ = env.step(action[0])\n",
    "            total_reward += reward\n",
    "            # step 3 -> if done then finish episode\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episodes_rewards.append(total_reward)\n",
    "    \n",
    "    return episodes_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managing Multiple Environments Simultaenously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvBatch:\n",
    "  # initializing multiple environments\n",
    "  def __init__(self, n_envs = 10):\n",
    "    self.envs = [make_env() for _ in range(n_envs)]\n",
    "\n",
    "  # resetting multiple environments\n",
    "  def reset(self):\n",
    "    _states = []\n",
    "    for env in self.envs:\n",
    "      _states.append(env.reset()[0])\n",
    "    return np.array(_states)\n",
    "\n",
    "  # stepping multiple agents in multiple environments\n",
    "  def step(self, actions):\n",
    "    next_states, rewards, dones, infos, _ = map(np.array, zip(*[env.step(a) for env, a in zip(self.envs, actions)]))\n",
    "    for i in range(len(self.envs)):\n",
    "      if dones[i]:\n",
    "        next_states[i] = self.envs[i].reset()[0] # we only want the state\n",
    "    return next_states, rewards, dones, infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training multiple agents in multiple environments (Asynchronous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3001 [00:00<?, ?it/s]/tmp/ipykernel_6997/2148395805.py:45: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
      "  1%|          | 19/3001 [00:10<19:01,  2.61it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward:  320.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1027/3001 [00:27<05:20,  6.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward:  580.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2023/3001 [00:48<02:58,  5.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward:  530.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3001/3001 [01:09<00:00, 43.01it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average agent reward:  680.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "env_batch = EnvBatch(number_environments)\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "with tqdm.trange(0, 3001) as progress_bar:\n",
    "  for i in progress_bar:\n",
    "    batch_actions = agent.act(batch_states)\n",
    "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
    "    batch_rewards *= 0.01\n",
    "    agent.step(batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
    "    batch_states = batch_next_states\n",
    "    if i % 1000 == 0:\n",
    "      print(\"Average agent reward: \", np.mean(evaluate(agent, env, n_episodes = 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the environments for visualization\n",
    "batch_states = env_batch.reset()\n",
    "\n",
    "# Record observations during agent's interaction for visualization\n",
    "recorded_observations = []\n",
    "\n",
    "# Run the agent in the environment for visualization\n",
    "for i in range(500):  # Assuming you want to visualize the first 500 steps\n",
    "    batch_actions = agent.act(batch_states)\n",
    "    batch_next_states, batch_rewards, batch_dones, _ = env_batch.step(batch_actions)\n",
    "    batch_states = batch_next_states\n",
    "\n",
    "    # Record the observations for visualization\n",
    "    for obs in batch_next_states:\n",
    "        recorded_observations.append(obs)\n",
    "\n",
    "# Use OpenCV to visualize recorded observations\n",
    "for obs in recorded_observations:\n",
    "    # Render the observation as an image\n",
    "    obs_image = cv2.cvtColor(obs.transpose(1, 2, 0), cv2.COLOR_RGB2BGR)\n",
    "    cv2.imshow('Kung Fu Environment', obs_image)\n",
    "    cv2.waitKey(50)  # Adjust the wait time (in milliseconds) as needed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Clean up OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
