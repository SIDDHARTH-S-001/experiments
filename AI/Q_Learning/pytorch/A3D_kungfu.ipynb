{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from gymnasium.spaces import Box\n",
    "from gymnasium import ObservationWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.distributions as distributions\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, action_size) -> None:\n",
    "        super(Network, self).__init__() # Activate inheritance\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=4,out_channels=32, kernel_size=(3, 3), stride=2) # we will have a stack of 4 grey scale images\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32,out_channels=32, kernel_size=(3, 3), stride=2)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=32,out_channels=32, kernel_size=(3, 3), stride=2)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        # 1st fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(512, 128)\n",
    "        # we'll have 2 output layers, 1 for the action values (actor) and other for state value (critic)\n",
    "        self.fc2a = torch.nn.Linear(128, action_size)\n",
    "        self.fc2s = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.conv1(state)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        action_values = self.fc2a(x)\n",
    "        state_value = self.fc2s(x)[0] # gets the value instead of the whole array\n",
    "\n",
    "        return action_values, state_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Shape:  (42, 42, 4)\n",
      "Number of actions:  14\n",
      "Action Names:  <bound method AtariEnv.get_action_meanings of <shimmy.atari_env.AtariEnv object at 0x7fae8b5f3520>>\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.core import Env\n",
    "\n",
    "\n",
    "class Preprocess(ObservationWrapper):\n",
    "    def __init__(self, env, height = 42, width = 42, crop = lambda img: img, dim_order = 'pytorch', color = False, n_frames = 4):\n",
    "        super(Preprocess, self).__init__(env)\n",
    "        self.img_size = (height, width)\n",
    "        self.crop = crop\n",
    "        self.dim_order = dim_order\n",
    "        self.color = color\n",
    "        self.frame_stack = n_frames\n",
    "        n_channels = 3 * n_frames if color else n_frames\n",
    "        obs_shape = {'tensorflow': (height, width, n_channels), 'pytorch': (height, width, n_channels)}[dim_order]\n",
    "        self.observation_space = Box(0.0, 1.0, obs_shape)\n",
    "        self.frames = np.zeros(obs_shape, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.frames = np.zeros_like(self.frames)\n",
    "        obs, info = self.env.reset()\n",
    "        self.update_buffer(obs)\n",
    "        \n",
    "        return self.frames, info\n",
    "    \n",
    "    def observation(self, img):\n",
    "        img = self.crop(img)\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        if not self.color:\n",
    "            if len(img.shape) == 3 and img.shap[2] == 3:\n",
    "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img.astype('float32') / 255.\n",
    "        \n",
    "        if self.color:\n",
    "            self.frames = np.roll(self.frames, shift=-3, axis=0)\n",
    "            self.frames[-3: ] = img\n",
    "        else:\n",
    "            self.frames = np.roll(self.frames, shift=-1, axis=0)\n",
    "            self.frames[-1: ] = img\n",
    "\n",
    "        return self.frames\n",
    "    \n",
    "    def update_buffer(self, obs):\n",
    "        self.frames = self.observation(obs)\n",
    "\n",
    "def make_env():\n",
    "    env = gym.make(\"KungFuMasterDeterministic-v4\", render_mode=\"rgb_array\")\n",
    "    env = Preprocess(env, height=42, width=42, crop=lambda img: img, dim_order='pytorch', color=False, n_frames=4)\n",
    "    \n",
    "    return env\n",
    "\n",
    "env = make_env()\n",
    "state_shape = env.observation_space.shape\n",
    "number_actions = env.action_space.n \n",
    "\n",
    "print('State Shape: ', state_shape)\n",
    "print('Number of actions: ', number_actions)\n",
    "print('Action Names: ', env.env.get_wrapper_attr('get_action_meanings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "discount_factor = 0.9\n",
    "number_environments = 10 # basically 10 different agents working seperately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing A3C class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, action_size):\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.action_size = action_size\n",
    "        # There is only one network, unlike DQN and DCQN which had a local and target network\n",
    "        self.network = Network(action_size).to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr = learning_rate)\n",
    "\n",
    "    def act(self, state):\n",
    "        # we always need an extra dimension for the batches\n",
    "        if state.ndim == 3:\n",
    "            state = [state] # because state variable is not a pytorch tensor yet\n",
    "        # now convert the state variable to a torch tensor\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        action_values, _ = self.network(state)\n",
    "        # implement Softman policy as the action-selection strategy\n",
    "        policy = F.softmax(action_values, dim=-1)\n",
    "\n",
    "        return np.array([np.random.choice(len(p), p = p) for p in policy.detach().cpu.numpy()])\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done): # will be done in batches\n",
    "        batch_size = state.shape[0]\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32, device=self.device)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32, device=self.device)\n",
    "        done = torch.tensor(done, dtype=torch.bool, device=self.device).to(torch.float32)\n",
    "        action_values, state_value = self.network(state)\n",
    "        _, next_state_value = self.network(next_state) # returns next action and next state, but we are not interested in the next action\n",
    "        \n",
    "        # Bellman equation\n",
    "        target_state_value = reward + discount_factor * next_state_value * (1 - done)\n",
    "        \n",
    "        # Implement Advantage\n",
    "        advamtage = target_state_value - state_value \n",
    "        \n",
    "        # actor loss calculation \n",
    "        probs = F.softmax(action_values, dim=-1)\n",
    "        logprobs = F.log_softmax(action_values, dim=-1)\n",
    "        entropy = -torch.sum(probs * logprobs, axis=-1)\n",
    "        batch_idx = np.arange(batch_size)\n",
    "        logp_actions = logprobs[batch_idx, action]\n",
    "        actor_loss = -(logp_actions * advamtage.detach()).mean() - 0.001 * entropy.mean() # the small value is multiplied to balance the importance of the entropy\n",
    "        \n",
    "        # critic loss (mse loss between target state value and the state value)\n",
    "        critic_loss = F.mse_loss(target_state_value.detach(), state_value)\n",
    "\n",
    "        total_loss = actor_loss + critic_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the A3C Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siddharth/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(action_size=number_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
